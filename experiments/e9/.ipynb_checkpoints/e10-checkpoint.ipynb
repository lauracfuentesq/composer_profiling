{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da995ba8-14b1-40d6-ba26-5175e2ce5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from miditok import REMI\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae38028f-fad3-451e-a3da-bafc259fe8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import features_vectors as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae5e4130-7212-4cb3-ad7f-a30ccb51fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('../../dataframes/train_set_3.csv')\n",
    "validation_data=pd.read_csv('../../dataframes/validation_set_3.csv')\n",
    "test_data=pd.read_csv('../../dataframes/test_set_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f860c2-b3d5-40cb-813e-f6b0f6d4c621",
   "metadata": {},
   "source": [
    "## 1. Get features vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71328b20-ffd4-4674-9990-f07257893fc5",
   "metadata": {},
   "source": [
    "### 1.1 Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5551ec47-e625-475f-863f-f0f03e0976ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs_share_magnet1/lafuente/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/miditok/midi_tokenizer.py:3252: UserWarning: The special token PAD_None is present twice in your configuration. Skipping its duplicated occurrence.\n",
      "  self.config = TokenizerConfig()\n",
      "/mnt/nfs_share_magnet1/lafuente/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/miditok/classes.py:702: UserWarning: The special token PAD_None is present twice in your configuration. Skipping its duplicated occurrence.\n",
      "  return cls(**input_dict, **kwargs)\n",
      "/mnt/nfs_share_magnet1/lafuente/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/miditok/classes.py:702: UserWarning: Argument nb_tempos has been renamed num_tempos, you should consider to updateyour code with this new argument name.\n",
      "  return cls(**input_dict, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "val_dataloader=utils.create_data_loader(validation_data)\n",
    "train_dataloader=utils.create_data_loader(train_data)\n",
    "test_dataloader=utils.create_data_loader(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6558569-e2d8-4c16-9702-854227399bd5",
   "metadata": {},
   "source": [
    "### 1.2 Get feature vectors from pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b266e69-1c68-4851-a33c-3652656db10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "train_feature_vectors=utils.get_feature_vectors(dataloader=train_dataloader,\n",
    "                                                dataframe=train_data,\n",
    "                                                set_type='train', \n",
    "                                                feature_tensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08c6414a-63a6-46de-b21c-f989177caea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_feature_vectors=utils.get_feature_vectors(dataloader=val_dataloader,\n",
    "                                                dataframe=validation_data,\n",
    "                                                set_type='val', \n",
    "                                                feature_tensors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3183b6-2b68-4ecf-a5da-206059acb9f1",
   "metadata": {},
   "source": [
    "## 2. Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6b17966-7fcc-4286-90e8-3c89edc28f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "seed = 42\n",
    "if seed is not None:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "class DatasetMLP(Dataset):\n",
    "\n",
    "    def __init__(self,data):\n",
    "        self.data=data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self,ind):\n",
    "        x=self.data[ind][:-1]\n",
    "        y=self.data[ind][-1]\n",
    "\n",
    "        return x,y\n",
    "\n",
    "class TestDataset(DatasetMLP):\n",
    "    def __getitem__(self,ind):\n",
    "        x=self.data[ind]\n",
    "        return x\n",
    "\n",
    "train_set_mlp=DatasetMLP(np.array(train_feature_vectors))\n",
    "val_set_mlp=DatasetMLP(np.array(val_feature_vectors))\n",
    "\n",
    "batch_size=20\n",
    "\n",
    "train_dataloder_mlp=DataLoader(train_set_mlp,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=True)  \n",
    "\n",
    "val_dataloder_mlp=DataLoader(val_set_mlp,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f903230b-95f2-4447-965e-cb79293f2abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 2)  # Output 2 classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_data in dataloader:\n",
    "            x, y = input_data\n",
    "            x = x.to(device).float()\n",
    "            y = y.to(device).long()\n",
    "\n",
    "            output = model(x)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "\n",
    "            batch_predictions = predicted.cpu().detach().numpy().tolist()\n",
    "            batch_true_labels = y.cpu().detach().numpy().tolist()\n",
    "\n",
    "            predictions.extend(batch_predictions)\n",
    "            true_labels.extend(batch_true_labels)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output, y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    # Compute average validation loss\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "\n",
    "    # Compute balanced accuracy\n",
    "    predictions = np.array(predictions)\n",
    "    true_labels = np.array(true_labels)\n",
    "    balanced_accuracy = balanced_accuracy_score(true_labels, predictions)\n",
    "\n",
    "    return balanced_accuracy, avg_loss, predictions, true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbd740b5-936f-4b6f-b460-a46617d914f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear): Linear(in_features=523776, out_features=2, bias=True)\n",
      ")\n",
      "Epoch 0 | Train Loss 122.27| Train Balanced Accuracy   0.55\n",
      "Epoch 0 | Validation Loss  50.77| Validation Balanced Accuracy:   0.61\n",
      "Epoch 1 | Train Loss  36.51| Train Balanced Accuracy   0.79\n",
      "Epoch 1 | Validation Loss  97.68| Validation Balanced Accuracy:   0.56\n",
      "Epoch 2 | Train Loss  17.61| Train Balanced Accuracy   0.89\n",
      "Epoch 2 | Validation Loss  91.94| Validation Balanced Accuracy:   0.54\n",
      "Epoch 3 | Train Loss   8.27| Train Balanced Accuracy   0.94\n",
      "Epoch 3 | Validation Loss 105.00| Validation Balanced Accuracy:   0.63\n",
      "Epoch 4 | Train Loss   7.40| Train Balanced Accuracy   0.95\n",
      "Epoch 4 | Validation Loss  90.29| Validation Balanced Accuracy:   0.62\n",
      "Epoch 5 | Train Loss  10.21| Train Balanced Accuracy   0.94\n",
      "Epoch 5 | Validation Loss 186.75| Validation Balanced Accuracy:   0.58\n",
      "Epoch 6 | Train Loss  11.62| Train Balanced Accuracy   0.93\n",
      "Epoch 6 | Validation Loss 147.78| Validation Balanced Accuracy:   0.61\n",
      "Epoch 7 | Train Loss   6.19| Train Balanced Accuracy   0.96\n",
      "Epoch 7 | Validation Loss 115.23| Validation Balanced Accuracy:   0.53\n",
      "Epoch 8 | Train Loss   3.74| Train Balanced Accuracy   0.97\n",
      "Epoch 8 | Validation Loss 127.26| Validation Balanced Accuracy:   0.57\n",
      "Epoch 9 | Train Loss   7.32| Train Balanced Accuracy   0.96\n",
      "Epoch 9 | Validation Loss 138.93| Validation Balanced Accuracy:   0.56\n",
      "Epoch 10 | Train Loss   5.35| Train Balanced Accuracy   0.97\n",
      "Epoch 10 | Validation Loss 147.96| Validation Balanced Accuracy:   0.56\n",
      "Epoch 11 | Train Loss   1.62| Train Balanced Accuracy   0.99\n",
      "Epoch 11 | Validation Loss 155.11| Validation Balanced Accuracy:   0.56\n",
      "Epoch 12 | Train Loss  12.71| Train Balanced Accuracy   0.93\n",
      "Epoch 12 | Validation Loss 201.85| Validation Balanced Accuracy:   0.55\n",
      "Epoch 13 | Train Loss   2.74| Train Balanced Accuracy   0.98\n",
      "Epoch 13 | Validation Loss 307.80| Validation Balanced Accuracy:   0.57\n",
      "Epoch 14 | Train Loss   2.01| Train Balanced Accuracy   0.99\n",
      "Epoch 14 | Validation Loss 191.82| Validation Balanced Accuracy:   0.56\n",
      "Epoch 15 | Train Loss   2.09| Train Balanced Accuracy   0.99\n",
      "Epoch 15 | Validation Loss 165.74| Validation Balanced Accuracy:   0.55\n",
      "Epoch 16 | Train Loss   2.64| Train Balanced Accuracy   0.99\n",
      "Epoch 16 | Validation Loss 165.72| Validation Balanced Accuracy:   0.54\n",
      "Epoch 17 | Train Loss   1.86| Train Balanced Accuracy   0.99\n",
      "Epoch 17 | Validation Loss 152.76| Validation Balanced Accuracy:   0.60\n",
      "Epoch 18 | Train Loss   1.99| Train Balanced Accuracy   0.99\n",
      "Epoch 18 | Validation Loss 158.57| Validation Balanced Accuracy:   0.53\n",
      "Epoch 19 | Train Loss   1.80| Train Balanced Accuracy   0.98\n",
      "Epoch 19 | Validation Loss 211.73| Validation Balanced Accuracy:   0.57\n",
      "Epoch 20 | Train Loss   5.52| Train Balanced Accuracy   0.97\n",
      "Epoch 20 | Validation Loss 192.07| Validation Balanced Accuracy:   0.56\n",
      "Epoch 21 | Train Loss   2.89| Train Balanced Accuracy   0.99\n",
      "Epoch 21 | Validation Loss 195.87| Validation Balanced Accuracy:   0.59\n",
      "Epoch 22 | Train Loss   1.85| Train Balanced Accuracy   0.99\n",
      "Epoch 22 | Validation Loss 162.17| Validation Balanced Accuracy:   0.59\n",
      "Epoch 23 | Train Loss   3.72| Train Balanced Accuracy   0.98\n",
      "Epoch 23 | Validation Loss 163.07| Validation Balanced Accuracy:   0.56\n",
      "Epoch 24 | Train Loss   2.03| Train Balanced Accuracy   0.99\n",
      "Epoch 24 | Validation Loss 227.26| Validation Balanced Accuracy:   0.53\n",
      "Epoch 25 | Train Loss   1.70| Train Balanced Accuracy   0.99\n",
      "Epoch 25 | Validation Loss 257.75| Validation Balanced Accuracy:   0.55\n",
      "Epoch 26 | Train Loss   0.18| Train Balanced Accuracy   1.00\n",
      "Epoch 26 | Validation Loss 203.44| Validation Balanced Accuracy:   0.61\n",
      "Epoch 27 | Train Loss   1.81| Train Balanced Accuracy   0.99\n",
      "Epoch 27 | Validation Loss 181.87| Validation Balanced Accuracy:   0.59\n",
      "Epoch 28 | Train Loss   1.87| Train Balanced Accuracy   0.99\n",
      "Epoch 28 | Validation Loss 263.83| Validation Balanced Accuracy:   0.55\n",
      "Epoch 29 | Train Loss   2.37| Train Balanced Accuracy   0.99\n",
      "Epoch 29 | Validation Loss 211.38| Validation Balanced Accuracy:   0.57\n",
      "Epoch 30 | Train Loss   1.72| Train Balanced Accuracy   0.99\n",
      "Epoch 30 | Validation Loss 206.92| Validation Balanced Accuracy:   0.62\n",
      "Epoch 31 | Train Loss   2.08| Train Balanced Accuracy   0.99\n",
      "Epoch 31 | Validation Loss 175.65| Validation Balanced Accuracy:   0.57\n",
      "Epoch 32 | Train Loss   2.26| Train Balanced Accuracy   0.98\n",
      "Epoch 32 | Validation Loss 201.08| Validation Balanced Accuracy:   0.59\n",
      "Epoch 33 | Train Loss   2.14| Train Balanced Accuracy   0.99\n",
      "Epoch 33 | Validation Loss 217.20| Validation Balanced Accuracy:   0.61\n",
      "Epoch 34 | Train Loss   2.28| Train Balanced Accuracy   0.99\n",
      "Epoch 34 | Validation Loss 245.20| Validation Balanced Accuracy:   0.60\n",
      "Epoch 35 | Train Loss   2.08| Train Balanced Accuracy   0.99\n",
      "Epoch 35 | Validation Loss 246.65| Validation Balanced Accuracy:   0.62\n",
      "Epoch 36 | Train Loss   6.16| Train Balanced Accuracy   0.98\n",
      "Epoch 36 | Validation Loss 239.80| Validation Balanced Accuracy:   0.57\n",
      "Epoch 37 | Train Loss   8.27| Train Balanced Accuracy   0.96\n",
      "Epoch 37 | Validation Loss 258.37| Validation Balanced Accuracy:   0.62\n",
      "Epoch 38 | Train Loss   7.02| Train Balanced Accuracy   0.97\n",
      "Epoch 38 | Validation Loss 429.64| Validation Balanced Accuracy:   0.57\n",
      "Epoch 39 | Train Loss   6.93| Train Balanced Accuracy   0.97\n",
      "Epoch 39 | Validation Loss 366.18| Validation Balanced Accuracy:   0.59\n",
      "Epoch 40 | Train Loss  10.45| Train Balanced Accuracy   0.97\n",
      "Epoch 40 | Validation Loss 384.93| Validation Balanced Accuracy:   0.57\n",
      "Epoch 41 | Train Loss   5.47| Train Balanced Accuracy   0.98\n",
      "Epoch 41 | Validation Loss 291.27| Validation Balanced Accuracy:   0.53\n",
      "Epoch 42 | Train Loss   2.12| Train Balanced Accuracy   0.99\n",
      "Epoch 42 | Validation Loss 279.70| Validation Balanced Accuracy:   0.56\n",
      "Epoch 43 | Train Loss   3.32| Train Balanced Accuracy   0.99\n",
      "Epoch 43 | Validation Loss 285.30| Validation Balanced Accuracy:   0.59\n",
      "Epoch 44 | Train Loss   2.87| Train Balanced Accuracy   0.99\n",
      "Epoch 44 | Validation Loss 266.81| Validation Balanced Accuracy:   0.59\n",
      "Epoch 45 | Train Loss   2.94| Train Balanced Accuracy   0.99\n",
      "Epoch 45 | Validation Loss 304.94| Validation Balanced Accuracy:   0.61\n",
      "Epoch 46 | Train Loss   1.03| Train Balanced Accuracy   1.00\n",
      "Epoch 46 | Validation Loss 347.59| Validation Balanced Accuracy:   0.59\n",
      "Epoch 47 | Train Loss   1.48| Train Balanced Accuracy   0.99\n",
      "Epoch 47 | Validation Loss 359.12| Validation Balanced Accuracy:   0.62\n",
      "Epoch 48 | Train Loss   1.95| Train Balanced Accuracy   0.99\n",
      "Epoch 48 | Validation Loss 276.07| Validation Balanced Accuracy:   0.63\n",
      "Epoch 49 | Train Loss   1.19| Train Balanced Accuracy   1.00\n",
      "Epoch 49 | Validation Loss 228.15| Validation Balanced Accuracy:   0.58\n",
      "Epoch 50 | Train Loss   0.96| Train Balanced Accuracy   0.99\n",
      "Epoch 50 | Validation Loss 249.41| Validation Balanced Accuracy:   0.64\n",
      "Epoch 51 | Train Loss   1.73| Train Balanced Accuracy   0.99\n",
      "Epoch 51 | Validation Loss 303.99| Validation Balanced Accuracy:   0.63\n",
      "Epoch 52 | Train Loss   1.75| Train Balanced Accuracy   0.99\n",
      "Epoch 52 | Validation Loss 287.41| Validation Balanced Accuracy:   0.62\n",
      "Epoch 53 | Train Loss   2.08| Train Balanced Accuracy   0.99\n",
      "Epoch 53 | Validation Loss 371.30| Validation Balanced Accuracy:   0.66\n",
      "Epoch 54 | Train Loss   2.48| Train Balanced Accuracy   0.99\n",
      "Epoch 54 | Validation Loss 269.78| Validation Balanced Accuracy:   0.66\n",
      "Epoch 55 | Train Loss   0.32| Train Balanced Accuracy   1.00\n",
      "Epoch 55 | Validation Loss 346.41| Validation Balanced Accuracy:   0.64\n",
      "Epoch 56 | Train Loss   0.45| Train Balanced Accuracy   1.00\n",
      "Epoch 56 | Validation Loss 381.86| Validation Balanced Accuracy:   0.65\n",
      "Epoch 57 | Train Loss   2.30| Train Balanced Accuracy   0.99\n",
      "Epoch 57 | Validation Loss 376.15| Validation Balanced Accuracy:   0.61\n",
      "Epoch 58 | Train Loss  24.14| Train Balanced Accuracy   0.96\n",
      "Epoch 58 | Validation Loss 624.31| Validation Balanced Accuracy:   0.58\n",
      "Epoch 59 | Train Loss  15.26| Train Balanced Accuracy   0.96\n",
      "Epoch 59 | Validation Loss 357.59| Validation Balanced Accuracy:   0.62\n",
      "Epoch 60 | Train Loss   5.66| Train Balanced Accuracy   0.98\n",
      "Epoch 60 | Validation Loss 546.49| Validation Balanced Accuracy:   0.61\n",
      "Epoch 61 | Train Loss   1.00| Train Balanced Accuracy   1.00\n",
      "Epoch 61 | Validation Loss 391.65| Validation Balanced Accuracy:   0.64\n",
      "Epoch 62 | Train Loss   4.54| Train Balanced Accuracy   0.99\n",
      "Epoch 62 | Validation Loss 474.30| Validation Balanced Accuracy:   0.58\n",
      "Epoch 63 | Train Loss   0.53| Train Balanced Accuracy   1.00\n",
      "Epoch 63 | Validation Loss 355.85| Validation Balanced Accuracy:   0.56\n",
      "Epoch 64 | Train Loss   0.63| Train Balanced Accuracy   0.99\n",
      "Epoch 64 | Validation Loss 564.02| Validation Balanced Accuracy:   0.58\n",
      "Epoch 65 | Train Loss   0.95| Train Balanced Accuracy   1.00\n",
      "Epoch 65 | Validation Loss 376.18| Validation Balanced Accuracy:   0.62\n",
      "Epoch 66 | Train Loss   2.12| Train Balanced Accuracy   1.00\n",
      "Epoch 66 | Validation Loss 423.34| Validation Balanced Accuracy:   0.60\n",
      "Epoch 67 | Train Loss   4.02| Train Balanced Accuracy   0.98\n",
      "Epoch 67 | Validation Loss 657.92| Validation Balanced Accuracy:   0.58\n",
      "Epoch 68 | Train Loss   2.23| Train Balanced Accuracy   1.00\n",
      "Epoch 68 | Validation Loss 441.64| Validation Balanced Accuracy:   0.61\n",
      "Epoch 69 | Train Loss   0.00| Train Balanced Accuracy   1.00\n",
      "Epoch 69 | Validation Loss 442.27| Validation Balanced Accuracy:   0.61\n",
      "Predictions: [0 1 1 ... 1 1 1]\n",
      "True Labels: [0 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Assuming optimizer is already defined\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "\n",
    "seed = 42\n",
    "if seed is not None:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "input_dim = 523776\n",
    "model = MLP(input_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "epochs = 70\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    for batch_num, input_data in enumerate(train_dataloder_mlp):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = input_data\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).long()  # Ensure y is of type long for CrossEntropyLoss\n",
    "\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert predictions to class labels (0 or 1)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        batch_predictions = predicted.cpu().detach().numpy().tolist()\n",
    "        batch_true_labels = y.cpu().detach().numpy().tolist()\n",
    "\n",
    "        predictions.extend(batch_predictions)\n",
    "        true_labels.extend(batch_true_labels)\n",
    "\n",
    "        #if batch_num % 40 == 0:\n",
    "        #    print('\\tEpoch %d | Batch %d | Loss %6.2f' % (epoch, batch_num, loss.item()))\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    train_balanced_accuracy = balanced_accuracy_score(true_labels, predictions)\n",
    "    print('Epoch %d | Train Loss %6.2f| Train Balanced Accuracy %6.2f' % (epoch, sum(losses)/len(losses) ,train_balanced_accuracy))\n",
    "    \n",
    "    val_balanced_accuracy, val_avg_loss, val_predictions, val_true_labels = evaluate(model, val_dataloder_mlp,criterion)\n",
    "    print('Epoch %d | Validation Loss %6.2f| Validation Balanced Accuracy: %6.2f' % (epoch, val_avg_loss, val_balanced_accuracy))\n",
    "\n",
    "\n",
    "# Convert predictions and true labels to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "# Example of using predictions and true labels\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"True Labels:\", true_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "712671ff-5c00-4517-a0fb-10d188343430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_accuracy_score(y_true=true_labels,y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2064c851-e345-4857-a66b-19c78c92893c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "computing feature tensors: 100%|██████████████| 103/103 [06:32<00:00,  3.81s/it]\n"
     ]
    }
   ],
   "source": [
    "test_feature_vectors=utils.get_feature_vectors(dataloader=test_dataloader,\n",
    "                                                dataframe=test_data,\n",
    "                                                set_type='test', \n",
    "                                                feature_tensors=False)\n",
    "\n",
    "test_set_mlp=DatasetMLP(np.array(test_feature_vectors))\n",
    "\n",
    "test_dataloder_mlp=DataLoader(test_set_mlp,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ee67db7-27a0-4d17-a3fc-b395f31bdacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_balanced_accuracy, loss, test_predictions, test_true_labels = evaluate(model, test_dataloder_mlp, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a9940711-b273-4b5b-aa45-e3bb106c33fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5456862446743762"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_balanced_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae2c0c37-f039-4b7a-8737-801a765205fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (linear): Linear(in_features=523776, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75822dbf-7bfa-4278-868f-ed294eaa162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_balanced_accuracy': train_balanced_accuracy\n",
    "        }, 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "69e6b00a-682d-4f66-bf37-2309087befc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "best_balanced_accuracy = checkpoint['best_balanced_accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "984bbeb7-72fc-4020-9fbe-415c7c93f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df_test_e10=pd.DataFrame(data={'labels':true_labels,'predictions':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6fec33a7-4709-42fa-b7ac-819eced12b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df_test_e10.to_csv('predictions_df_test_e10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2e86633c-b05d-4006-ac94-d05c9db87da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df_val_e10=pd.DataFrame(data={'labels':val_true_labels,'predictions':val_predictions})\n",
    "predictions_df_val_e10.to_csv('predictions_df_val_e10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2161ca70-cc5b-42ca-8ee1-ccaabb858f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symbolic_music",
   "language": "python",
   "name": "symbolic_music"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
