{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da995ba8-14b1-40d6-ba26-5175e2ce5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from miditok import REMI\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae38028f-fad3-451e-a3da-bafc259fe8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import features_vectors as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de30df5a-00b9-46a2-a100-8d8de337d90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'features_vectors' from '/mnt/nfs_share_magnet1/lafuente/symbolic_music/author-profiling/experiments/e14/features_vectors.py'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae5e4130-7212-4cb3-ad7f-a30ccb51fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('../../train data/piano_merged_scores_chunks_train_set.csv')\n",
    "validation_data=pd.read_csv('../../train data/piano_merged_scores_chunks_validation_set.csv')\n",
    "test_data=pd.read_csv('../../train data/piano_merged_scores_chunks_test_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f860c2-b3d5-40cb-813e-f6b0f6d4c621",
   "metadata": {},
   "source": [
    "## 1. Get features vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71328b20-ffd4-4674-9990-f07257893fc5",
   "metadata": {},
   "source": [
    "### 1.1 Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5551ec47-e625-475f-863f-f0f03e0976ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs_share_magnet1/lafuente/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/miditok/midi_tokenizer.py:3252: UserWarning: The special token PAD_None is present twice in your configuration. Skipping its duplicated occurrence.\n",
      "  self.config = TokenizerConfig()\n",
      "/mnt/nfs_share_magnet1/lafuente/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/miditok/classes.py:702: UserWarning: The special token PAD_None is present twice in your configuration. Skipping its duplicated occurrence.\n",
      "  return cls(**input_dict, **kwargs)\n",
      "/mnt/nfs_share_magnet1/lafuente/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/miditok/classes.py:702: UserWarning: Argument nb_tempos has been renamed num_tempos, you should consider to updateyour code with this new argument name.\n",
      "  return cls(**input_dict, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "val_dataloader=utils.create_data_loader(scores_df=validation_data,paths_column_name='piano_merged_type0_chunks_paths')\n",
    "train_dataloader=utils.create_data_loader(scores_df=train_data,paths_column_name='piano_merged_type0_chunks_paths')\n",
    "test_dataloader=utils.create_data_loader(scores_df=test_data,paths_column_name='piano_merged_type0_chunks_paths')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6558569-e2d8-4c16-9702-854227399bd5",
   "metadata": {},
   "source": [
    "### 1.2 Get feature vectors from pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b266e69-1c68-4851-a33c-3652656db10f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_feature_vectors\u001b[38;5;241m=\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_feature_vectors(dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[1;32m      2\u001b[0m                                                 dataframe\u001b[38;5;241m=\u001b[39mtrain_data,\n\u001b[1;32m      3\u001b[0m                                                 set_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m                                                 feature_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/symbolic_music/author-profiling/experiments/e14/features_vectors.py:79\u001b[0m, in \u001b[0;36mget_feature_vectors\u001b[0;34m(dataloader, dataframe, set_type, feature_tensors)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm \n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     82\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_default_device(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/symbolic_music/author-profiling/experiments/e14/features_vectors.py:30\u001b[0m, in \u001b[0;36mCustomDatasetMIDI.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 30\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(idx)\n\u001b[1;32m     31\u001b[0m     item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]  \u001b[38;5;66;03m# Add labels to the item dictionary\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "File \u001b[0;32m~/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/miditok/pytorch_data/datasets.py:201\u001b[0m, in \u001b[0;36mDatasetMIDI.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     score \u001b[38;5;241m=\u001b[39m Score(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles_paths[idx])\n\u001b[0;32m--> 201\u001b[0m     tseq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize_score(score)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m# If not one_token_stream, we only take the first track/sequence\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m tseq\u001b[38;5;241m.\u001b[39mids \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mone_token_stream \u001b[38;5;28;01melse\u001b[39;00m tseq[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mids\n",
      "File \u001b[0;32m~/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/miditok/pytorch_data/datasets.py:258\u001b[0m, in \u001b[0;36mDatasetMIDI._tokenize_score\u001b[0;34m(self, score)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m tokseq:\n\u001b[0;32m--> 258\u001b[0m         seq\u001b[38;5;241m.\u001b[39mids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_token_ids(\n\u001b[1;32m    259\u001b[0m             seq\u001b[38;5;241m.\u001b[39mids,\n\u001b[1;32m    260\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_len,\n\u001b[1;32m    261\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbos_token_id \u001b[38;5;28;01mif\u001b[39;00m add_bos_token \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    262\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meos_token_id \u001b[38;5;28;01mif\u001b[39;00m add_eos_token \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    263\u001b[0m             enforce_eos_token_if_seq_len_exceed_lim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    264\u001b[0m         )\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokseq\n",
      "File \u001b[0;32m~/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/miditok/pytorch_data/datasets.py:55\u001b[0m, in \u001b[0;36m_DatasetABC._preprocess_token_ids\u001b[0;34m(token_ids, max_seq_len, bos_token_id, eos_token_id, enforce_eos_token_if_seq_len_exceed_lim)\u001b[0m\n\u001b[1;32m     53\u001b[0m     token_ids\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, bos_token_id)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     56\u001b[0m         eos_token_id \u001b[38;5;241m=\u001b[39m [eos_token_id] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(token_ids[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     57\u001b[0m     token_ids\u001b[38;5;241m.\u001b[39mappend(eos_token_id)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_feature_vectors=utils.get_feature_vectors(dataloader=train_dataloader,\n",
    "                                                dataframe=train_data,\n",
    "                                                set_type='train', \n",
    "                                                feature_tensors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08c6414a-63a6-46de-b21c-f989177caea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_feature_vectors=utils.get_feature_vectors(dataloader=val_dataloader,\n",
    "                                                dataframe=validation_data,\n",
    "                                                set_type='val', \n",
    "                                                feature_tensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "088ec7c0-ccb0-4f33-847f-f7030df96a27",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'REMI' object has no attribute 'token_to_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 112\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features_df\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Ensure the dataframes train_data, validation_data, and test_data are defined\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m create_data_loader(scores_df\u001b[38;5;241m=\u001b[39mvalidation_data, paths_column_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpiano_merged_type0_chunks_paths\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    113\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m create_data_loader(scores_df\u001b[38;5;241m=\u001b[39mtrain_data, paths_column_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpiano_merged_type0_chunks_paths\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    114\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m create_data_loader(scores_df\u001b[38;5;241m=\u001b[39mtest_data, paths_column_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpiano_merged_type0_chunks_paths\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 46\u001b[0m, in \u001b[0;36mcreate_data_loader\u001b[0;34m(scores_df, paths_column_name)\u001b[0m\n\u001b[1;32m     43\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m REMI\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNatooz/Maestro-REMI-bpe20k\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Ensure the eos_token_id is available\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m eos_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtoken_to_id\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBOS_None\u001b[39m\u001b[38;5;124m'\u001b[39m, tokenizer\u001b[38;5;241m.\u001b[39meos_token_id)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Initialize the dataset\u001b[39;00m\n\u001b[1;32m     49\u001b[0m dataset \u001b[38;5;241m=\u001b[39m CustomDatasetMIDI(\n\u001b[1;32m     50\u001b[0m     files_paths\u001b[38;5;241m=\u001b[39mmidi_paths,\n\u001b[1;32m     51\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     eos_token_id\u001b[38;5;241m=\u001b[39meos_token_id\n\u001b[1;32m     56\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'REMI' object has no attribute 'token_to_id'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from miditok import REMI\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed = 42\n",
    "if seed is not None:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define a custom DatasetMIDI subclass\n",
    "class CustomDatasetMIDI(DatasetMIDI):\n",
    "    def __init__(self, files_paths, labels, tokenizer, max_seq_len, bos_token_id, eos_token_id):\n",
    "        super().__init__(files_paths, tokenizer, max_seq_len, bos_token_id, eos_token_id)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = super().__getitem__(idx)\n",
    "        item[\"labels\"] = self.labels[idx]  # Add labels to the item dictionary\n",
    "        return item\n",
    "\n",
    "def create_data_loader(scores_df, paths_column_name):\n",
    "    midi_paths = []\n",
    "    labels = []\n",
    "    paths = scores_df[paths_column_name]\n",
    "\n",
    "    for i, score in enumerate(paths):\n",
    "        midi_paths.append(Path(score))\n",
    "        integer_label = 0 if scores_df['composer_gender'][i] == 'Male' else 1\n",
    "        labels.append(torch.tensor(integer_label))\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = REMI.from_pretrained(\"Natooz/Maestro-REMI-bpe20k\")\n",
    "\n",
    "    # Ensure the eos_token_id is available\n",
    "    eos_token_id = tokenizer.token_to_id.get('BOS_None', tokenizer.eos_token_id)\n",
    "\n",
    "    # Initialize the dataset\n",
    "    dataset = CustomDatasetMIDI(\n",
    "        files_paths=midi_paths,\n",
    "        labels=labels,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_len=1024,\n",
    "        bos_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=eos_token_id\n",
    "    )\n",
    "\n",
    "    # Initialize the collator\n",
    "    collator = DataCollator(tokenizer.pad_token_id)\n",
    "\n",
    "    data_loader = DataLoader(dataset=dataset, collate_fn=collator, batch_size=len(scores_df))\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "def get_feature_vectors(dataloader, dataframe, set_type, feature_tensors):\n",
    "    from transformers import AutoModelForCausalLM\n",
    "    \n",
    "    torch.set_default_device(\"cpu\")\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"Natooz/Maestro-REMI-bpe20k\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=\"auto\",\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "    \n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "    last_hidden_state_list = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"computing feature tensors\"):\n",
    "        # Ensure input_ids is available in batch and not empty\n",
    "        if 'input_ids' in batch and batch['input_ids'].shape[0] > 0:\n",
    "            for i in range(0, batch['input_ids'].shape[0], 5):\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(batch['input_ids'][i:i+5])\n",
    "                hidden_states = outputs.hidden_states\n",
    "                last_hidden_state = hidden_states[-1]\n",
    "                last_hidden_state_list.append(last_hidden_state)\n",
    "\n",
    "    #torch.save(last_hidden_state_list, f'tensor_list_{set_type}.pt')\n",
    "    loader_tensor_list = last_hidden_state_list\n",
    "    \n",
    "    # Flatten feature vectors for MLP\n",
    "    flattened_tensor_list = []\n",
    "    for batch in range(0, len(loader_tensor_list)):\n",
    "        for element in range(0, len(loader_tensor_list[batch])):\n",
    "            flat_tensor = torch.flatten(loader_tensor_list[batch][element])\n",
    "            flattened_tensor_list.append(flat_tensor)\n",
    "\n",
    "    numpy_arrays = [item.numpy() for item in flattened_tensor_list]\n",
    "    features_df = pd.DataFrame(data={'feature_vectors': numpy_arrays})\n",
    "    features_df = features_df['feature_vectors'].apply(pd.Series)\n",
    "    \n",
    "    composer_gender = dataframe['composer_gender'].apply(lambda x: 0 if x == 'Male' else 1)\n",
    "    features_df['label'] = composer_gender\n",
    "\n",
    "    return features_df\n",
    "\n",
    "# Example usage\n",
    "# Ensure the dataframes train_data, validation_data, and test_data are defined\n",
    "val_dataloader = create_data_loader(scores_df=validation_data, paths_column_name='piano_merged_type0_chunks_paths')\n",
    "train_dataloader = create_data_loader(scores_df=train_data, paths_column_name='piano_merged_type0_chunks_paths')\n",
    "test_dataloader = create_data_loader(scores_df=test_data, paths_column_name='piano_merged_type0_chunks_paths')\n",
    "\n",
    "train_feature_vectors = get_feature_vectors(dataloader=train_dataloader, dataframe=train_data, set_type='train', feature_tensors=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3183b6-2b68-4ecf-a5da-206059acb9f1",
   "metadata": {},
   "source": [
    "## 2. Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6b17966-7fcc-4286-90e8-3c89edc28f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "seed = 42\n",
    "if seed is not None:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "class DatasetMLP(Dataset):\n",
    "\n",
    "    def __init__(self,data):\n",
    "        self.data=data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self,ind):\n",
    "        x=self.data[ind][:-1]\n",
    "        y=self.data[ind][-1]\n",
    "\n",
    "        return x,y\n",
    "\n",
    "class TestDataset(DatasetMLP):\n",
    "    def __getitem__(self,ind):\n",
    "        x=self.data[ind]\n",
    "        return x\n",
    "\n",
    "train_set_mlp=DatasetMLP(np.array(train_feature_vectors))\n",
    "val_set_mlp=DatasetMLP(np.array(val_feature_vectors))\n",
    "\n",
    "batch_size=20\n",
    "\n",
    "train_dataloder_mlp=DataLoader(train_set_mlp,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=True)  \n",
    "\n",
    "val_dataloder_mlp=DataLoader(val_set_mlp,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f903230b-95f2-4447-965e-cb79293f2abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#class MLP(nn.Module):\n",
    "#    def __init__(self, input_dim):\n",
    "#        super(MLP, self).__init__()\n",
    "#        self.linear = nn.Linear(input_dim, 2)  # Output 2 classes\n",
    "    \n",
    "#    def forward(self, x):\n",
    "#        out = self.linear(x)\n",
    "#        return out\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1=128, hidden_dim2=64, hidden_dim3=32, output_dim=2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        out = self.fc4(x)\n",
    "        return out\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_data in dataloader:\n",
    "            x, y = input_data\n",
    "            x = x.to(device).float()\n",
    "            y = y.to(device).long()\n",
    "\n",
    "            output = model(x)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "\n",
    "            batch_predictions = predicted.cpu().detach().numpy().tolist()\n",
    "            batch_true_labels = y.cpu().detach().numpy().tolist()\n",
    "\n",
    "            predictions.extend(batch_predictions)\n",
    "            true_labels.extend(batch_true_labels)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output, y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    # Compute average validation loss\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "\n",
    "    # Compute balanced accuracy\n",
    "    predictions = np.array(predictions)\n",
    "    true_labels = np.array(true_labels)\n",
    "    balanced_accuracy = balanced_accuracy_score(true_labels, predictions)\n",
    "\n",
    "    return balanced_accuracy, avg_loss, predictions, true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbd740b5-936f-4b6f-b460-a46617d914f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=523264, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Epoch 0 | Train Loss   3.52| Train Balanced Accuracy   0.56\n",
      "Epoch 0 | Validation Loss   4.17| Validation Balanced Accuracy:   0.54\n",
      "Epoch 1 | Train Loss   1.79| Train Balanced Accuracy   0.77\n",
      "Epoch 1 | Validation Loss   1.97| Validation Balanced Accuracy:   0.72\n",
      "Epoch 2 | Train Loss   0.50| Train Balanced Accuracy   0.90\n",
      "Epoch 2 | Validation Loss   2.61| Validation Balanced Accuracy:   0.69\n",
      "Epoch 3 | Train Loss   0.09| Train Balanced Accuracy   0.97\n",
      "Epoch 3 | Validation Loss   2.35| Validation Balanced Accuracy:   0.71\n",
      "Epoch 4 | Train Loss   0.01| Train Balanced Accuracy   1.00\n",
      "Epoch 4 | Validation Loss   2.33| Validation Balanced Accuracy:   0.70\n",
      "Epoch 5 | Train Loss   0.00| Train Balanced Accuracy   1.00\n",
      "Epoch 5 | Validation Loss   2.29| Validation Balanced Accuracy:   0.71\n",
      "Epoch 6 | Train Loss   0.00| Train Balanced Accuracy   1.00\n",
      "Epoch 6 | Validation Loss   2.29| Validation Balanced Accuracy:   0.72\n",
      "Epoch 7 | Train Loss   0.00| Train Balanced Accuracy   1.00\n",
      "Epoch 7 | Validation Loss   2.29| Validation Balanced Accuracy:   0.71\n",
      "Epoch 8 | Train Loss   0.00| Train Balanced Accuracy   1.00\n",
      "Epoch 8 | Validation Loss   2.29| Validation Balanced Accuracy:   0.71\n",
      "Epoch 9 | Train Loss   0.00| Train Balanced Accuracy   1.00\n",
      "Epoch 9 | Validation Loss   2.29| Validation Balanced Accuracy:   0.71\n",
      "Predictions: [1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0\n",
      " 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 1\n",
      " 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1\n",
      " 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0\n",
      " 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
      " 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1\n",
      " 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0\n",
      " 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 1\n",
      " 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1\n",
      " 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0\n",
      " 0 0 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0\n",
      " 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 1 1 1 0 0\n",
      " 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 0\n",
      " 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0\n",
      " 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1\n",
      " 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1\n",
      " 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 1\n",
      " 0 0 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1\n",
      " 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1\n",
      " 1 0 0 1 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0]\n",
      "True Labels: [1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0\n",
      " 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 1\n",
      " 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1\n",
      " 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0\n",
      " 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
      " 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1\n",
      " 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0\n",
      " 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 1\n",
      " 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1\n",
      " 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0\n",
      " 0 0 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0\n",
      " 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 1 1 1 0 0\n",
      " 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 0\n",
      " 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0\n",
      " 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1\n",
      " 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1\n",
      " 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 1\n",
      " 0 0 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1\n",
      " 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1\n",
      " 1 0 0 1 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "\n",
    "\n",
    "seed = 42\n",
    "if seed is not None:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "input_dim = 523264\n",
    "model = MLP(input_dim).to(device)\n",
    "\n",
    "initial_lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Assuming optimizer is already defined\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "print(model)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "model.train()\n",
    "train_avg_loss_list=[]\n",
    "val_avg_loss_list=[]\n",
    "train_balanced_accuracy_list=[]\n",
    "val_balanced_accuracy_list=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    for batch_num, input_data in enumerate(train_dataloder_mlp):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = input_data\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).long()  # Ensure y is of type long for CrossEntropyLoss\n",
    "\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert predictions to class labels (0 or 1)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        batch_predictions = predicted.cpu().detach().numpy().tolist()\n",
    "        batch_true_labels = y.cpu().detach().numpy().tolist()\n",
    "\n",
    "        predictions.extend(batch_predictions)\n",
    "        true_labels.extend(batch_true_labels)\n",
    "\n",
    "        #if batch_num % 40 == 0:\n",
    "        #    print('\\tEpoch %d | Batch %d | Loss %6.2f' % (epoch, batch_num, loss.item()))\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    train_balanced_accuracy = balanced_accuracy_score(true_labels, predictions)\n",
    "    train_avg_loss=sum(losses)/len(losses) \n",
    "    print('Epoch %d | Train Loss %6.2f| Train Balanced Accuracy %6.2f' % (epoch, train_avg_loss ,train_balanced_accuracy))\n",
    "    \n",
    "    val_balanced_accuracy, val_avg_loss, val_predictions, val_true_labels = evaluate(model, val_dataloder_mlp,criterion)\n",
    "    print('Epoch %d | Validation Loss %6.2f| Validation Balanced Accuracy: %6.2f' % (epoch, val_avg_loss, val_balanced_accuracy))\n",
    "\n",
    "    train_avg_loss_list.append(train_avg_loss)\n",
    "    val_avg_loss_list.append(val_avg_loss)\n",
    "    \n",
    "    train_balanced_accuracy_list.append(train_balanced_accuracy)\n",
    "    val_balanced_accuracy_list.append(val_balanced_accuracy)\n",
    "\n",
    "\n",
    "# Convert predictions and true labels to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "# Example of using predictions and true labels\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"True Labels:\", true_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "712671ff-5c00-4517-a0fb-10d188343430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_accuracy_score(y_true=true_labels,y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2064c851-e345-4857-a66b-19c78c92893c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_feature_vectors=utils.get_feature_vectors(dataloader=test_dataloader,\n",
    "                                                dataframe=test_data,\n",
    "                                                set_type='test', \n",
    "                                                feature_tensors=True)\n",
    "\n",
    "test_set_mlp=DatasetMLP(np.array(test_feature_vectors))\n",
    "\n",
    "test_dataloder_mlp=DataLoader(test_set_mlp,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ee67db7-27a0-4d17-a3fc-b395f31bdacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_balanced_accuracy, loss, test_predictions, test_true_labels = evaluate(model, test_dataloder_mlp, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9940711-b273-4b5b-aa45-e3bb106c33fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.82188746123172"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_balanced_accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae2c0c37-f039-4b7a-8737-801a765205fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df=pd.DataFrame(data={'train_avg_loss':train_avg_loss_list,\n",
    "                            'train_balanced_accuracy':train_balanced_accuracy_list,\n",
    "                            'val_avg_loss':val_avg_loss_list,\n",
    "                            'val_balanced_accuracy':val_balanced_accuracy_list})\n",
    "metrics_df.to_csv('metrics_df_e11.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75822dbf-7bfa-4278-868f-ed294eaa162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_balanced_accuracy': train_balanced_accuracy\n",
    "        }, 'best_model_e11.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69e6b00a-682d-4f66-bf37-2309087befc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_e11.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/torch/serialization.py:1025\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1024\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[1;32m   1026\u001b[0m                      map_location,\n\u001b[1;32m   1027\u001b[0m                      pickle_module,\n\u001b[1;32m   1028\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[1;32m   1029\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1031\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/torch/serialization.py:1446\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1445\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1446\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m   1448\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1449\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1451\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/torch/serialization.py:1416\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1416\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/torch/serialization.py:1390\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1389\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1390\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1391\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1392\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1395\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/torch/serialization.py:390\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 390\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(storage, location)\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mcuda(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/symbolic_music/lib/python3.12/site-packages/torch/_utils.py:114\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_type(indices, values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('best_model_e11.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "best_balanced_accuracy = checkpoint['best_balanced_accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "984bbeb7-72fc-4020-9fbe-415c7c93f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df_test_e11=pd.DataFrame(data={'labels':true_labels,'predictions':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fec33a7-4709-42fa-b7ac-819eced12b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df_test_e11.to_csv('predictions_df_test_e11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e86633c-b05d-4006-ac94-d05c9db87da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df_val_e11=pd.DataFrame(data={'labels':val_true_labels,'predictions':val_predictions})\n",
    "predictions_df_val_e11.to_csv('predictions_df_val_e11.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symbolic_music",
   "language": "python",
   "name": "symbolic_music"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
